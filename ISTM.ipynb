{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Belvinbarasa/Eng.barasa/blob/main/ISTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Importing Necessary Libraries**"
      ],
      "metadata": {
        "id": "G_MA4jsmN5S_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2UjiHBZyEVg"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code imports essential libraries for building and working with deep learning models in TensorFlow.\n",
        "\n",
        "tensorflow and keras are used to define, train, and evaluate neural networks, while numpy is a library for numerical computations, often used to handle arrays and matrices, which are fundamental to machine learning tasks.\n",
        "\n",
        "These libraries provide the tools necessary to build, optimize, and work with deep learning models efficiently."
      ],
      "metadata": {
        "id": "fqnS36jwOLo-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the buily-in IMDB dataset\n",
        "imdb=keras.datasets.imdb"
      ],
      "metadata": {
        "id": "l3seB-hP2KR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This loads the built-in IMDB dataset from Keras, which is a collection of movie reviews labeled as either positive or negative.\n",
        "\n",
        "The dataset is commonly used for sentiment analysis tasks. By assigning keras.datasets.imdb to the variable imdb, you can easily access the dataset's training and test sets, which are typically used for training machine learning models for binary classification\n",
        "\n"
      ],
      "metadata": {
        "id": "pZ5cGoV4OQQg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the vocabulary size and maximum sequence length\n",
        "vocab_size=10000\n",
        "max_length=250"
      ],
      "metadata": {
        "id": "VHa15ydM83iq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code sets two important parameters for preprocessing the IMDB dataset.\n",
        "\n",
        "**vocab_size=10000** means that only the top 10,000 most frequent words in the dataset will be considered as features, effectively limiting the vocabulary to the most common terms.\n",
        "\n",
        " **max_length=250** specifies that the movie reviews will be padded or truncated to a fixed length of 250 words. This ensures that all input sequences have the same length, which is necessary for feeding data into a neural network."
      ],
      "metadata": {
        "id": "1srXzmfnOjEG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Loading the dataset**"
      ],
      "metadata": {
        "id": "BArLIgYYOwHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "(x_train,y_train),(x_test,y_test)=imdb.load_data(num_words=vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xC2UA-pU9Jpo",
        "outputId": "c51c08bd-b1de-421b-c6e5-086449fd682b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code loads the IMDB dataset for training and testing, limiting the vocabulary to the top 10,000 most frequent words, as specified by num_words=vocab_size.\n",
        "\n",
        "The imdb.load_data() function returns two tuples: (x_train, y_train) for the training set and (x_test, y_test) for the test set. x_train and x_test contain the tokenized movie reviews (represented as sequences of integers corresponding to word indices), while y_train and y_test contain the labels (0 for negative reviews, 1 for positive reviews).\n",
        "\n",
        "This prepares the data for training and evaluating a machine learning mode"
      ],
      "metadata": {
        "id": "kLdipkv2PA8-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pad the sequences to have the same length\n",
        "x_train=keras.preprocessing.sequence.pad_sequences(x_train,maxlen=max_length)\n",
        "x_test=keras.preprocessing.sequence.pad_sequences(x_test,maxlen=max_length)"
      ],
      "metadata": {
        "id": "HyVGNIau9anY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code ensures that all input sequences (movie reviews) have the same length by padding or truncating them to a fixed size of 250 words, as specified by maxlen=max_length.\n",
        "\n",
        " The keras.preprocessing.sequence.pad_sequences() function is applied to both the training data (x_train) and the test data (x_test).\n",
        "\n",
        " If a review is shorter than 250 words, it is padded with zeros at the beginning; if it's longer, it is truncated to the first 250 words. This step is crucial because neural networks require inputs of consistent size for efficient processing."
      ],
      "metadata": {
        "id": "ktbOeZDNPLtF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Building the LTSM model**"
      ],
      "metadata": {
        "id": "xc2IN3GfPddn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the LSTM model\n",
        "model=keras.Sequential([\n",
        "    keras.layers.Embedding(vocab_size,32),\n",
        "    keras.layers.LSTM(32),\n",
        "    keras.layers.Dense(1,activation='sigmoid')\n",
        "  ])"
      ],
      "metadata": {
        "id": "I1auNmUT9_WI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the LSTM model\n",
        "model=keras.Sequential([\n",
        "    keras.layers.Embedding(vocab_size,32),\n",
        "    keras.layers.LSTM(32, input_shape=(x_train.shape[1], 32)), # Specify input_shape for LSTM\n",
        "    keras.layers.Dense(1,activation='sigmoid')\n",
        "  ])"
      ],
      "metadata": {
        "id": "XpqoN20qQOOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code builds a simple LSTM (Long Short-Term Memory) model using Keras for sentiment analysis. The model consists of three layers:\n",
        "\n",
        "1. **Embedding Layer:** The Embedding(vocab_size, 32) layer transforms the integer-encoded words in the input sequences into dense vectors of size 32. It maps each of the top 10,000 words (defined by vocab_size) to a 32-dimensional vector, allowing the model to learn better representations for the words.\n",
        "\n",
        "**2. LSTM Layer:** The LSTM(32) layer is a type of recurrent neural network (RNN) designed to capture long-term dependencies in sequences. It contains 32 units, which help the model learn patterns in the sequence of words in the reviews.\n",
        "\n",
        "**3. Dense Layer:** The Dense(1, activation='sigmoid') layer is the output layer. It has a single neuron with a sigmoid activation function, which is used for binary classification (positive or negative review).\n",
        "\n",
        "This model architecture is designed to process sequences of text and classify them into two categories: positive or negative."
      ],
      "metadata": {
        "id": "Y26Cm0-TPyvu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**COMPILING THE MODEL**"
      ],
      "metadata": {
        "id": "3CJnDfGGQpXu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "               loss='binary_crossentrophy',\n",
        "               metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "aaLP6CGa-tKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training the model**"
      ],
      "metadata": {
        "id": "Ce-nGZi6QtYu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code trains the LSTM model on the training data (x_train and y_train) for 10 epochs with a batch size of 32. The validation_split=0.2 parameter means that 20% of the training data will be used for validation during training. The model will be evaluated on this validation data after each epoch to monitor its performance and generalization ability.\n",
        "\n",
        " The fit() method returns a history object, which stores the training and validation metrics (such as accuracy and loss) over the epochs. This allows you to track how the model's performance evolves over time."
      ],
      "metadata": {
        "id": "yrcKcRvoSOn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train,y_train,\n",
        "                  epochs=10,\n",
        "                  batch_size=32,\n",
        "                  validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "Eat_DSTnCXLu",
        "outputId": "72438d69-18c6-4a69-d216-b1e2e4fb8b17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d3acbad0571f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m history = model.fit(x_train,y_train,\n\u001b[0m\u001b[1;32m      3\u001b[0m                   \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                   \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                   validation_split=0.2)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This output shows the training progress over 10 epochs for the LSTM model. Each line corresponds to an epoch and provides several metrics:\n",
        "\n",
        "1. accuracy: The model's accuracy on the training data for that epoch.\n",
        "\n",
        "2. loss: The training loss, which measures the difference between the model's predictions and the actual labels. Lower values are better.\n",
        "\n",
        "3. val_accuracy: The model's accuracy on the validation set after that epoch. This helps monitor how well the model generalizes to unseen data.\n",
        "\n",
        "4. val_loss: The validation loss, which measures how well the model's predictions match the actual labels in the validation set.\n",
        "\n",
        "Key points from the output:\n",
        "\n",
        "- Epoch 1 starts with a training accuracy of 70.34% and validation accuracy of 83.24%.\n",
        "\n",
        "- Epoch 2 sees a significant improvement in training accuracy (89.93%) and a slight increase in validation accuracy (87.80%).\n",
        "\n",
        "- Training accuracy continues to rise steadily, reaching 98.00% by Epoch 10, while validation accuracy fluctuates slightly, peaking at 87.12% in Epoch 6 and ending at 84.98% in Epoch 10.\n",
        "\n",
        "- The validation loss starts at 0.3732 in Epoch 1, improves initially, but then increases in later epochs, suggesting some overfitting (the model fits the training data well but may struggle to generalize on validation data).\n",
        "\n",
        "Overall, the model shows strong training performance, but the increasing validation loss and slight fluctuations in validation accuracy suggest some potential overfitting, especially after Epoch 6."
      ],
      "metadata": {
        "id": "kcAzcAkKRVJt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Evaluation**"
      ],
      "metadata": {
        "id": "ceULGiFxQz52"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code evaluates the trained LSTM model on the test data (x_test and y_test) using the evaluate() function. The function returns two values: test_loss and test_acc.\n",
        "\n",
        "- test_loss represents how well the model's predictions match the true labels on the test set, measured using the loss function (in this case, likely binary cross-entropy, given the binary classification task).\n",
        "\n",
        "- test_acc represents the model's accuracy on the test set, i.e., the proportion of correctly classified reviews.\n",
        "\n",
        "Finally, it prints the test accuracy (test_acc), which provides an indication of how well the model performs on unseen data."
      ],
      "metadata": {
        "id": "OC4Qgp1NQ7_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the model\n",
        "test_loss,test_acc=model.evaluate(x_test,y_test)\n",
        "print('Test accuracy:',test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqo1ya-bBS55",
        "outputId": "5d336e35-4051-4d1a-c688-33b9d7cee9ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.8447 - loss: 0.5740\n",
            "Test accuracy: 0.8463199734687805\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This output indicates that the model has been evaluated on the test set. The 782/782 shows that there are 782 batches in the test data, and the evaluation took 4 seconds with an average of 5 milliseconds per step.\n",
        "\n",
        "**The accuracy:** 0.8447 indicates that the model achieved an accuracy of approximately 84.47% on the test data during the evaluation.\n",
        "\n",
        "**The loss:** 0.5740 represents the value of the loss function, which quantifies how far the model's predictions are from the actual labels.\n",
        "\n",
        "Finally, the printed Test accuracy: 0.8463 confirms that the model's accuracy on the test set is approximately 84.63%. This suggests the model is performing well in classifying the sentiment of movie reviews, achieving a good balance between accuracy and loss."
      ],
      "metadata": {
        "id": "lYkpziMNRJ_u"
      }
    }
  ]
}