{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Belvinbarasa/Eng.barasa/blob/main/LSTM_TRANSLATION.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7UtID3vIzDq"
      },
      "source": [
        "## **Step 1: Importing Necessary libraries**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fN7M8FWEGyMD"
      },
      "source": [
        "This code imports essential libraries for working with data and building deep learning models.\n",
        "\n",
        "It first imports pandas (aliased as pd), which is a powerful data manipulation and analysis library commonly used for handling structured datasets in machine learning workflows.\n",
        "\n",
        " Next, it imports the keras module from TensorFlow, which provides a high-level API for building and training deep learning models. The layers module from tensorflow.\n",
        "\n",
        " keras is also imported, allowing users to define various types of neural network layers such as dense (fully connected), convolutional, recurrent, and more.\n",
        "\n",
        " This setup is typically used for developing deep learning models, especially in tasks like classification, regression, and anomaly detection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Owu9iBlP9yk8"
      },
      "outputs": [],
      "source": [
        "import numpy as pd\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mmd2NbzMJQDa"
      },
      "source": [
        "## **Step 2: Loading the dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwjT9VCjHGUw"
      },
      "source": [
        "This code loads and preprocesses the IMDB movie reviews dataset for sentiment analysis using TensorFlow's Keras module.\n",
        "\n",
        "It first sets max_features to 10,000, defining the vocabulary size by considering only the most frequent words, and max_len to 200, ensuring that reviews are trimmed or padded to a fixed length of 200 words for uniformity in input size.\n",
        "\n",
        " The dataset is then loaded using keras.datasets.imdb.load_data(num_words=max_features), which returns tokenized training and test sets, with X_train and X_test containing sequences of word indices and y_train and y_test containing corresponding sentiment labels (positive or negative).\n",
        "\n",
        " To ensure all reviews have a consistent length, the sequences are padded using keras.preprocessing.sequence.pad_sequences(), which either truncates longer reviews or pads shorter ones with zeros up to max_len. This preprocessing step is essential for feeding data into neural networks, which require fixed-length inputs for efficient training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLwHuiAm_fMg",
        "outputId": "db295111-1e78-4293-af46-66b62bc48e8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "from re import X\n",
        "# Load the IMDB dataset\n",
        "max_features = 10000 # Number of words to consideras features\n",
        "max_len = 200 # Trim reviews after ths number of words\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = keras.datasets.imdb.load_data(num_words=max_features)\n",
        "# Pad sequences to a fixed length\n",
        "X = train = keras.preprocessing.sequence.pad_sequences(X_train, maxlen=max_len)\n",
        "X_test = keras.preprocessing.sequence.pad_sequences(X_test, maxlen=max_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxXaVF9bJamg"
      },
      "source": [
        "## **Step 3: Building the GRU Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xWbovG2HggH"
      },
      "source": [
        "The given code constructs a Gated Recurrent Unit (GRU) model using TensorFlow's Keras Sequential API for processing sequential data, such as text in the IMDB sentiment analysis task. The model consists of three key layers: an Embedding layer, a GRU layer, and a Dense output layer.\n",
        "\n",
        " The Embedding layer maps the integer-encoded words from the dataset into dense vector representations of size 128, enabling the model to learn meaningful word embeddings.\n",
        "\n",
        " The GRU layer, which is a type of recurrent neural network (RNN), has 128 units and includes both dropout and recurrent_dropout set to 0.2 to prevent overfitting by randomly dropping connections during training.\n",
        "\n",
        " Finally, the Dense layer with a single neuron and a sigmoid activation function is used for binary classification, outputting a probability score that determines whether a review is positive or negative. The model is well-suited for natural language processing (NLP) tasks where sequential dependencies play a crucial role."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lkp0XtQaAGH-"
      },
      "outputs": [],
      "source": [
        "# Build the GRU model\n",
        "model = keras.Sequential([\n",
        "    layers.Embedding(max_features, 128), # Added comma here\n",
        "    layers.GRU(128, dropout=0.2, recurrent_dropout=0.2), # Added comma here\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "]) # This is the correct closing parenthesis for the list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4R2r6UWdJl2w"
      },
      "source": [
        "## **Step 4: Compiling the Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5xhXMI8H7cZ"
      },
      "source": [
        "The code compiles the GRU model by specifying the loss function (binary_crossentropy), the optimizer (adam), and the evaluation metric (accuracy), which are commonly used for binary classification tasks like sentiment analysis.\n",
        "\n",
        "The model is then trained using the fit() method, where X (the padded training sequences) and y_train (the training labels) are used as input. The training process is conducted in batches of 32 samples for 5 epochs, with validation data provided by the test set (X_test and y_test) to evaluate the model's performance after each epoch.\n",
        "\n",
        "This setup allows the model to learn from the training data while also monitoring its generalization ability on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJ5Hc_WYAI4v",
        "outputId": "cf9b45d8-1e57-4877-b3af-01c1f58707a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m473s\u001b[0m 594ms/step - accuracy: 0.9866 - loss: 0.0428 - val_accuracy: 0.8602 - val_loss: 0.5123\n",
            "Epoch 2/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m500s\u001b[0m 592ms/step - accuracy: 0.9919 - loss: 0.0261 - val_accuracy: 0.8563 - val_loss: 0.5625\n",
            "Epoch 3/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m502s\u001b[0m 592ms/step - accuracy: 0.9954 - loss: 0.0167 - val_accuracy: 0.8531 - val_loss: 0.6561\n",
            "Epoch 4/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m499s\u001b[0m 588ms/step - accuracy: 0.9964 - loss: 0.0122 - val_accuracy: 0.8550 - val_loss: 0.7564\n",
            "Epoch 5/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m503s\u001b[0m 590ms/step - accuracy: 0.9975 - loss: 0.0096 - val_accuracy: 0.8546 - val_loss: 0.7363\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x783b397328d0>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Train the model\n",
        "batch_size = 32\n",
        "epochs = 5\n",
        "\n",
        "model.fit(X, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_data=(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-b8S7crJu2h"
      },
      "source": [
        "## **Step 5: Evaluating the model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biLjJ9V6IOSH"
      },
      "source": [
        "The code evaluates the trained GRU model's performance on the test data (X_test and y_test) using the evaluate() method, which computes the loss and accuracy.\n",
        "\n",
        "The loss represents how well the model's predictions match the true labels, while accuracy indicates the proportion of correct predictions.\n",
        "\n",
        "After evaluation, the test loss and accuracy are printed to provide insights into the model's effectiveness in predicting sentiment on unseen data. The results are displayed with four decimal places for precision."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0Tvc-laaDocM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sGTzMD7nBRo0",
        "outputId": "98e7a8e7-6ede-428e-9a6b-917dcd36dcef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 68ms/step - accuracy: 0.8522 - loss: 0.7550\n",
            "Test loss: 0.7363\n",
            "Test accuracy: 0.8546\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test,)\n",
        "print(f'Test loss: {loss:.4f}')\n",
        "print(f'Test accuracy: {accuracy:.4f}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}